{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from tokenizer.tokenizer import create_numerate_array\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailers_csv = \"/Volumes/Seagate/natasha-diploma/trailers.csv\"\n",
    "trailers_df = pd.read_csv(trailers_csv, index_col=None, header=0)\n",
    "trailers_df.drop(trailers_df.columns[trailers_df.columns.str.contains('Unnamed',case = False)],axis = 1, inplace = True)\n",
    "display(trailers_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_csv = \"/Volumes/Seagate/natasha-diploma/sessions.csv\"\n",
    "sessions_df = pd.read_csv(sessions_csv, index_col=None, header=0)\n",
    "sessions_df.drop(sessions_df.columns[sessions_df.columns.str.contains('Unnamed',case = False)],axis = 1, inplace = True)\n",
    "display(sessions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_id = sessions_df.to_numpy()\n",
    "copy = movies_id\n",
    "np.random.shuffle(copy)\n",
    "movies_id = np.concatenate((np.array(movies_id).flatten(), copy.flatten()))\n",
    "                           \n",
    "vocab = np.unique(movies_id)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "print(vocab_len)\n",
    "print(movies_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVideoInfo(info_df, mean=True):\n",
    "    tokenized_hue = create_numerate_array(info_df['hue'].to_numpy())\n",
    "    counts = np.argmax(np.bincount(tokenized_hue))\n",
    "    \n",
    "    scenes_array = [\n",
    "        info_df['brightness'].to_numpy(),\n",
    "        info_df['colorfulness'].to_numpy(),\n",
    "        info_df['energy'].to_numpy(),\n",
    "        info_df['tempo'].to_numpy(),\n",
    "        info_df['amplitude'].to_numpy(),\n",
    "        info_df['mfcc'].to_numpy()\n",
    "    ]\n",
    "    scenes_array = np.nan_to_num(scenes_array, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    scaler = MinMaxScaler((-1, 1))\n",
    "    scenes_array = scaler.fit_transform(scenes_array)\n",
    "    \n",
    "    if mean:\n",
    "        new_scenes_array = []\n",
    "        for i in range(len(scenes_array)):\n",
    "            new_scenes_array.append(np.around(np.mean(scenes_array[i]), 3))\n",
    "            \n",
    "        scenes_array = new_scenes_array\n",
    "        \n",
    "    scenes_array.append(counts)\n",
    "    \n",
    "    return scenes_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {}\n",
    "scenes_array = []\n",
    "del_array_name = []\n",
    "info_folder = \"/Volumes/Seagate/natasha-diploma/videoinfo\"\n",
    "for i in range(0, len(movies_id)):\n",
    "    try:\n",
    "        info_csv = info_folder + '/' + movies_id[i] + '.csv'\n",
    "        info_df =  pd.read_csv(info_csv, index_col=None, header=0)\n",
    "        info[movies_id[i]] = getVideoInfo(info_df.iloc[: , 4:])\n",
    "        scenes_array.append(info[movies_id[i]])\n",
    "    except:\n",
    "        del_array_name.append(movies_id[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = create_numerate_array(movies_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(scenes_array).shape)\n",
    "print(np.array(tokenized).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_size = 1\n",
    "id_embed_size = 7\n",
    "vocab_size = 366\n",
    "rnn_units = 366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, id_size, id_embed_size, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.id_embedding = tf.keras.layers.Embedding(vocab_size, id_embed_size)\n",
    "        self.normalization = tf.keras.layers.BatchNormalization()\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation=activations.relu)\n",
    "\n",
    "    def call(self, _input, states=None, return_state=False, training=False):\n",
    "        ids = self.id_embedding(_input[0])\n",
    "        features = self.normalization(_input[1])\n",
    "        x = self.concat([ids, features])\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def predict(self, x, history, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10,\n",
    "    workers=1, use_multiprocessing=False,):\n",
    "        predictions = super().predict(x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\n",
    "        predicted_values = []\n",
    "        for i in range(len(predictions)):\n",
    "            output = predictions[-1][0]\n",
    "            mask = []\n",
    "            for i in range(vocab_size):\n",
    "                if i + 1 in history:\n",
    "                    mask.append(1)\n",
    "                else:\n",
    "                    mask.append(0)\n",
    "            masked_prediction = np.ma.array(output, mask=mask)\n",
    "            predicted_values.append(np.argmax(masked_prediction) + 1)\n",
    "            \n",
    "        \n",
    "        return predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenized[:3000]\n",
    "test_tokenized = tokenized[3000:4200]\n",
    "\n",
    "train_scenes = scenes_array[:3000]\n",
    "test_scenes = scenes_array[3000:4200]\n",
    "\n",
    "ids_ds = tf.data.Dataset.from_tensor_slices(train_tokenized)\n",
    "features_ds = tf.data.Dataset.from_tensor_slices(train_scenes)\n",
    "\n",
    "ids_ds_test = tf.data.Dataset.from_tensor_slices(test_tokenized)\n",
    "features_ds_test = tf.data.Dataset.from_tensor_slices(test_scenes)\n",
    "\n",
    "ds_test = tf.data.Dataset.zip((ids_ds_test, features_ds_test))\n",
    "\n",
    "ds = tf.data.Dataset.zip((ids_ds, features_ds))\n",
    "seq_length = 10\n",
    "examples_per_epoch = len(tokenized) // (seq_length + 1)\n",
    "\n",
    "sequences = ds.batch(seq_length + 1, drop_remainder=True)\n",
    "ids_seq = ids_ds.batch(seq_length + 1, drop_remainder=True)\n",
    "features_seq = features_ds.batch(seq_length + 1, drop_remainder=True)\n",
    "sequences_test = ds_test.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_split_input_target(id_sequence, feature_sequence):\n",
    "    input_text = (id_sequence[:-1], feature_sequence[:-1])\n",
    "    target_text = (id_sequence[1:])\n",
    "    return input_text, target_text\n",
    "\n",
    "def split_input_target(seq):\n",
    "    input_text = seq[:-1]\n",
    "    target_text = seq[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "ds = sequences.map(full_split_input_target)\n",
    "ids_ds = ids_seq.map(split_input_target)\n",
    "features_ds = features_seq.map(split_input_target)\n",
    "ds_test = sequences_test.map(full_split_input_target)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 1\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "ds = (\n",
    "    ds\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "\n",
    "ids_ds = (\n",
    "    ids_ds\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "features_ds = (\n",
    "    features_ds\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "ds_test = (\n",
    "    ds_test\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(TEST_BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    id_size=id_size,\n",
    "    id_embed_size=id_embed_size,\n",
    "    rnn_units=rnn_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _input, _output in ds.take(1):\n",
    "    example_batch_predictions = model(_input)\n",
    "    print(example_batch_predictions, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "    sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "\n",
    "    print(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "history = model.fit(ds, validation_data=ds_test, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(ds_test, batch_size=TEST_BATCH_SIZE, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _input, _output in ds_test.take(1):\n",
    "    values = model.predict(_input, _input[0].numpy())\n",
    "    print(values, _output.numpy()[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[_output.numpy()[0][-1] - 1])\n",
    "print(vocab[values[0] - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVideoInfoByName(name):\n",
    "    info_csv = info_folder + '/' + name + '.csv'\n",
    "    info_df =  pd.read_csv(info_csv, index_col=None, header=0)\n",
    "    return getVideoInfo(info_df.iloc[: , 4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_info = getVideoInfoByName(vocab[_output.numpy()[0][-1] - 1])\n",
    "output_info = getVideoInfoByName(vocab[values[0] - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_info = np.nan_to_num(true_info, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "output_info = np.nan_to_num(output_info, copy=False, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(id_embed_size):\n",
    "    print(true_info[i])\n",
    "    print(output_info[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_info = []\n",
    "for i in range(10):\n",
    "    print(i, getVideoInfoByName(vocab[_input[0].numpy()[0][i] - 1]))\n",
    "    print(\"OUTPUT:\", output_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Точность модели')\n",
    "plt.ylabel('точность')\n",
    "plt.xlabel('эпоха')\n",
    "plt.legend(['тренировочные данные', 'тестовые данные'], loc='upper left')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Потери модели')\n",
    "plt.ylabel('потери')\n",
    "plt.xlabel('эпоха')\n",
    "plt.legend(['тренировочные данные', 'тестовые данные'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
