{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>duration</th>\n",
       "      <th>release_date</th>\n",
       "      <th>restrict</th>\n",
       "      <th>orig_title</th>\n",
       "      <th>kinopoisk_id</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>kinopoisk_rating</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>categories</th>\n",
       "      <th>genres</th>\n",
       "      <th>properties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60432</td>\n",
       "      <td>Философия Фила</td>\n",
       "      <td>5816.0</td>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Phil</td>\n",
       "      <td>960568</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.86</td>\n",
       "      <td>Фил – стоматолог, страдающий от депрессии. Его...</td>\n",
       "      <td>{Фильмы}</td>\n",
       "      <td>{Драмы,Зарубежные,Комедии}</td>\n",
       "      <td>{ \"Место действия\" : [\"США\"], \"Качества\" : [\"д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64219</td>\n",
       "      <td>Зубная фея</td>\n",
       "      <td>5123.0</td>\n",
       "      <td>2019-04-02</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Tooth Fairy</td>\n",
       "      <td>1126022</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Старая женщина начинает рассказывать легенду о...</td>\n",
       "      <td>{Фильмы}</td>\n",
       "      <td>{Зарубежные,Триллеры,Ужасы}</td>\n",
       "      <td>{ \"Место действия\" : [\"не определено\"], \"Качес...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65407</td>\n",
       "      <td>Проснись</td>\n",
       "      <td>4667.0</td>\n",
       "      <td>2019-01-19</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Wake Up</td>\n",
       "      <td>1069713</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Врач-психиатр исследует дневник девушки, котор...</td>\n",
       "      <td>{Фильмы}</td>\n",
       "      <td>{Боевики,Зарубежные,Криминал,Триллеры,Ужасы}</td>\n",
       "      <td>{ \"Аудитория\" : [\"для взрослых\",\"для подготовл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75149</td>\n",
       "      <td>[4k] Адская кухня</td>\n",
       "      <td>6148.0</td>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>16.0</td>\n",
       "      <td>The Kitchen</td>\n",
       "      <td>1117988</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.50</td>\n",
       "      <td>Смотрите в 4К! Три жены ирландских гангстеров,...</td>\n",
       "      <td>{Фильмы}</td>\n",
       "      <td>{Боевики,Драмы,Зарубежные,Криминал,По комиксам}</td>\n",
       "      <td>{ \"Настроение\" : [\"захватывающие\",\"авантюрный\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75629</td>\n",
       "      <td>[4k] Покемон. Детектив Пикачу</td>\n",
       "      <td>6013.0</td>\n",
       "      <td>2019-05-02</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Pokémon Detective Pikachu</td>\n",
       "      <td>994864</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.56</td>\n",
       "      <td>Смотрите в 4K! Элитный детектив Пикачу отправл...</td>\n",
       "      <td>{Для детей,Фильмы}</td>\n",
       "      <td>{Детективы,Зарубежные,Комедии,Семейные,Фильмы,...</td>\n",
       "      <td>{ \"Аудитория\" : [\"для детей 13-16 лет\",\"9 лет\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                          title  duration release_date  restrict  \\\n",
       "0  60432                 Философия Фила    5816.0   2019-04-05      18.0   \n",
       "1  64219                     Зубная фея    5123.0   2019-04-02      18.0   \n",
       "2  65407                       Проснись    4667.0   2019-01-19      16.0   \n",
       "3  75149              [4k] Адская кухня    6148.0   2019-08-09      16.0   \n",
       "4  75629  [4k] Покемон. Детектив Пикачу    6013.0   2019-05-02      12.0   \n",
       "\n",
       "                  orig_title  kinopoisk_id  imdb_rating  kinopoisk_rating  \\\n",
       "0                       Phil        960568          5.4              5.86   \n",
       "1                Tooth Fairy       1126022          2.2              0.00   \n",
       "2                    Wake Up       1069713          2.9              0.00   \n",
       "3                The Kitchen       1117988          5.5              5.50   \n",
       "4  Pokémon Detective Pikachu        994864          6.6              6.56   \n",
       "\n",
       "                                            synopsis          categories  \\\n",
       "0  Фил – стоматолог, страдающий от депрессии. Его...            {Фильмы}   \n",
       "1  Старая женщина начинает рассказывать легенду о...            {Фильмы}   \n",
       "2  Врач-психиатр исследует дневник девушки, котор...            {Фильмы}   \n",
       "3  Смотрите в 4К! Три жены ирландских гангстеров,...            {Фильмы}   \n",
       "4  Смотрите в 4K! Элитный детектив Пикачу отправл...  {Для детей,Фильмы}   \n",
       "\n",
       "                                              genres  \\\n",
       "0                         {Драмы,Зарубежные,Комедии}   \n",
       "1                        {Зарубежные,Триллеры,Ужасы}   \n",
       "2       {Боевики,Зарубежные,Криминал,Триллеры,Ужасы}   \n",
       "3    {Боевики,Драмы,Зарубежные,Криминал,По комиксам}   \n",
       "4  {Детективы,Зарубежные,Комедии,Семейные,Фильмы,...   \n",
       "\n",
       "                                          properties  \n",
       "0  { \"Место действия\" : [\"США\"], \"Качества\" : [\"д...  \n",
       "1  { \"Место действия\" : [\"не определено\"], \"Качес...  \n",
       "2  { \"Аудитория\" : [\"для взрослых\",\"для подготовл...  \n",
       "3  { \"Настроение\" : [\"захватывающие\",\"авантюрный\"...  \n",
       "4  { \"Аудитория\" : [\"для детей 13-16 лет\",\"9 лет\"...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>title_x</th>\n",
       "      <th>duration</th>\n",
       "      <th>release_date</th>\n",
       "      <th>restrict</th>\n",
       "      <th>orig_title</th>\n",
       "      <th>kinopoisk_id</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>kinopoisk_rating</th>\n",
       "      <th>...</th>\n",
       "      <th>emotions</th>\n",
       "      <th>Place of act</th>\n",
       "      <th>Qualities</th>\n",
       "      <th>Time of act</th>\n",
       "      <th>Based on</th>\n",
       "      <th>Audience</th>\n",
       "      <th>Mood</th>\n",
       "      <th>Subgenre</th>\n",
       "      <th>About</th>\n",
       "      <th>Theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Философия Фила</td>\n",
       "      <td>5816.0</td>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Phil</td>\n",
       "      <td>960568</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.86</td>\n",
       "      <td>...</td>\n",
       "      <td>{'neutral': 0.5312193632125854, 'negative': 0....</td>\n",
       "      <td>['США']</td>\n",
       "      <td>['дебютный']</td>\n",
       "      <td>['21-й век', 'новейшее время']</td>\n",
       "      <td>['оригинального сценария']</td>\n",
       "      <td>['для молодёжи', 'для взрослых']</td>\n",
       "      <td>['грустный', 'депрессивный', 'трогательный']</td>\n",
       "      <td>['Драмы', 'Комедийные']</td>\n",
       "      <td>['преследования', 'самозванцы', 'расследование...</td>\n",
       "      <td>['развлекательные', 'психологический']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Зубная фея</td>\n",
       "      <td>5123.0</td>\n",
       "      <td>2019-04-02</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Tooth Fairy</td>\n",
       "      <td>1126022</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>{'negative': 0.4688006341457367, 'skip': 0.392...</td>\n",
       "      <td>['не определено']</td>\n",
       "      <td>['зрелищные', 'жанровый', 'неожиданный финал']</td>\n",
       "      <td>['не определено']</td>\n",
       "      <td>['эпоса и легенд', 'оригинального сценария']</td>\n",
       "      <td>['для взрослых', 'для мужчин', 'для подготовле...</td>\n",
       "      <td>['трагичные', 'мрачные', 'шокирующие', 'захват...</td>\n",
       "      <td>['ужасы', 'триллер']</td>\n",
       "      <td>['преследования', 'страхи', 'дети', 'молодежь'...</td>\n",
       "      <td>['развлекательные']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Проснись</td>\n",
       "      <td>4667.0</td>\n",
       "      <td>2019-01-19</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Wake Up</td>\n",
       "      <td>1069713</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>{'positive': 0.3140605390071869, 'negative': 0...</td>\n",
       "      <td>['не определено']</td>\n",
       "      <td>['жанровый', 'дебютный']</td>\n",
       "      <td>['новейшее время']</td>\n",
       "      <td>['оригинального сценария']</td>\n",
       "      <td>['для взрослых', 'для подготовленного зрителя'...</td>\n",
       "      <td>['захватывающие', 'жестокие', 'напряженные', '...</td>\n",
       "      <td>['Драмы', 'Боевики', 'триллер']</td>\n",
       "      <td>['психотерапевты', 'кровь', 'тайна смерти', 'п...</td>\n",
       "      <td>['психологический', 'развлекательные']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>[4k] Адская кухня</td>\n",
       "      <td>6148.0</td>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>16.0</td>\n",
       "      <td>The Kitchen</td>\n",
       "      <td>1117988</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.50</td>\n",
       "      <td>...</td>\n",
       "      <td>{'neutral': 0.4765896201133728, 'negative': 0....</td>\n",
       "      <td>['большой город', 'США']</td>\n",
       "      <td>['жанровый', 'остросюжетные']</td>\n",
       "      <td>['20-й век']</td>\n",
       "      <td>['комиксов']</td>\n",
       "      <td>['для мужчин', 'для женщин', 'для взрослых']</td>\n",
       "      <td>['захватывающие', 'авантюрный', 'жестокие', 'н...</td>\n",
       "      <td>['Боевики', 'Драмы', 'криминальные']</td>\n",
       "      <td>['враги или вражда', 'женщины', 'антигерои', '...</td>\n",
       "      <td>['авантюрные', 'гангстерская']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>[4k] Покемон. Детектив Пикачу</td>\n",
       "      <td>6013.0</td>\n",
       "      <td>2019-05-02</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Pokémon Detective Pikachu</td>\n",
       "      <td>994864</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.56</td>\n",
       "      <td>...</td>\n",
       "      <td>{'neutral': 0.4532718360424042, 'negative': 0....</td>\n",
       "      <td>['вымышленная страна (город)', 'большой город']</td>\n",
       "      <td>['частично анимационный', 'зрелищные', 'кассов...</td>\n",
       "      <td>['21-й век']</td>\n",
       "      <td>['оригинального сценария', 'сериалов']</td>\n",
       "      <td>['для детей 13-16 лет', '9 лет', '8 лет', '7-1...</td>\n",
       "      <td>['авантюрный', 'захватывающие', 'динамичный', ...</td>\n",
       "      <td>['детские', 'Приключенческие', 'Фантастические...</td>\n",
       "      <td>['притяжение противоположностей', 'риск', 'пре...</td>\n",
       "      <td>['развлекательные', 'образовательный']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                        title_x  duration  \\\n",
       "0           0             0                 Философия Фила    5816.0   \n",
       "1           1             4                     Зубная фея    5123.0   \n",
       "2           2             5                       Проснись    4667.0   \n",
       "3           3             6              [4k] Адская кухня    6148.0   \n",
       "4           4             7  [4k] Покемон. Детектив Пикачу    6013.0   \n",
       "\n",
       "  release_date  restrict                 orig_title  kinopoisk_id  \\\n",
       "0   2019-04-05      18.0                       Phil        960568   \n",
       "1   2019-04-02      18.0                Tooth Fairy       1126022   \n",
       "2   2019-01-19      16.0                    Wake Up       1069713   \n",
       "3   2019-08-09      16.0                The Kitchen       1117988   \n",
       "4   2019-05-02      12.0  Pokémon Detective Pikachu        994864   \n",
       "\n",
       "   imdb_rating  kinopoisk_rating  ...  \\\n",
       "0          5.4              5.86  ...   \n",
       "1          2.2              0.00  ...   \n",
       "2          2.9              0.00  ...   \n",
       "3          5.5              5.50  ...   \n",
       "4          6.6              6.56  ...   \n",
       "\n",
       "                                            emotions  \\\n",
       "0  {'neutral': 0.5312193632125854, 'negative': 0....   \n",
       "1  {'negative': 0.4688006341457367, 'skip': 0.392...   \n",
       "2  {'positive': 0.3140605390071869, 'negative': 0...   \n",
       "3  {'neutral': 0.4765896201133728, 'negative': 0....   \n",
       "4  {'neutral': 0.4532718360424042, 'negative': 0....   \n",
       "\n",
       "                                      Place of act  \\\n",
       "0                                          ['США']   \n",
       "1                                ['не определено']   \n",
       "2                                ['не определено']   \n",
       "3                         ['большой город', 'США']   \n",
       "4  ['вымышленная страна (город)', 'большой город']   \n",
       "\n",
       "                                           Qualities  \\\n",
       "0                                       ['дебютный']   \n",
       "1     ['зрелищные', 'жанровый', 'неожиданный финал']   \n",
       "2                           ['жанровый', 'дебютный']   \n",
       "3                      ['жанровый', 'остросюжетные']   \n",
       "4  ['частично анимационный', 'зрелищные', 'кассов...   \n",
       "\n",
       "                      Time of act  \\\n",
       "0  ['21-й век', 'новейшее время']   \n",
       "1               ['не определено']   \n",
       "2              ['новейшее время']   \n",
       "3                    ['20-й век']   \n",
       "4                    ['21-й век']   \n",
       "\n",
       "                                       Based on  \\\n",
       "0                    ['оригинального сценария']   \n",
       "1  ['эпоса и легенд', 'оригинального сценария']   \n",
       "2                    ['оригинального сценария']   \n",
       "3                                  ['комиксов']   \n",
       "4        ['оригинального сценария', 'сериалов']   \n",
       "\n",
       "                                            Audience  \\\n",
       "0                   ['для молодёжи', 'для взрослых']   \n",
       "1  ['для взрослых', 'для мужчин', 'для подготовле...   \n",
       "2  ['для взрослых', 'для подготовленного зрителя'...   \n",
       "3       ['для мужчин', 'для женщин', 'для взрослых']   \n",
       "4  ['для детей 13-16 лет', '9 лет', '8 лет', '7-1...   \n",
       "\n",
       "                                                Mood  \\\n",
       "0       ['грустный', 'депрессивный', 'трогательный']   \n",
       "1  ['трагичные', 'мрачные', 'шокирующие', 'захват...   \n",
       "2  ['захватывающие', 'жестокие', 'напряженные', '...   \n",
       "3  ['захватывающие', 'авантюрный', 'жестокие', 'н...   \n",
       "4  ['авантюрный', 'захватывающие', 'динамичный', ...   \n",
       "\n",
       "                                            Subgenre  \\\n",
       "0                            ['Драмы', 'Комедийные']   \n",
       "1                               ['ужасы', 'триллер']   \n",
       "2                    ['Драмы', 'Боевики', 'триллер']   \n",
       "3               ['Боевики', 'Драмы', 'криминальные']   \n",
       "4  ['детские', 'Приключенческие', 'Фантастические...   \n",
       "\n",
       "                                               About  \\\n",
       "0  ['преследования', 'самозванцы', 'расследование...   \n",
       "1  ['преследования', 'страхи', 'дети', 'молодежь'...   \n",
       "2  ['психотерапевты', 'кровь', 'тайна смерти', 'п...   \n",
       "3  ['враги или вражда', 'женщины', 'антигерои', '...   \n",
       "4  ['притяжение противоположностей', 'риск', 'пре...   \n",
       "\n",
       "                                    Theme  \n",
       "0  ['развлекательные', 'психологический']  \n",
       "1                     ['развлекательные']  \n",
       "2  ['психологический', 'развлекательные']  \n",
       "3          ['авантюрные', 'гангстерская']  \n",
       "4  ['развлекательные', 'образовательный']  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60432</td>\n",
       "      <td>174457</td>\n",
       "      <td>184174</td>\n",
       "      <td>111847</td>\n",
       "      <td>172257</td>\n",
       "      <td>227812</td>\n",
       "      <td>250885</td>\n",
       "      <td>174558</td>\n",
       "      <td>212559</td>\n",
       "      <td>214967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65407</td>\n",
       "      <td>225043</td>\n",
       "      <td>138191</td>\n",
       "      <td>177902</td>\n",
       "      <td>176620</td>\n",
       "      <td>225371</td>\n",
       "      <td>190349</td>\n",
       "      <td>147590</td>\n",
       "      <td>181198</td>\n",
       "      <td>138245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75149</td>\n",
       "      <td>60432</td>\n",
       "      <td>133587</td>\n",
       "      <td>148584</td>\n",
       "      <td>186787</td>\n",
       "      <td>222725</td>\n",
       "      <td>212087</td>\n",
       "      <td>174369</td>\n",
       "      <td>423155</td>\n",
       "      <td>431690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75629</td>\n",
       "      <td>205372</td>\n",
       "      <td>186058</td>\n",
       "      <td>171392</td>\n",
       "      <td>160160</td>\n",
       "      <td>184174</td>\n",
       "      <td>172257</td>\n",
       "      <td>75630</td>\n",
       "      <td>226747</td>\n",
       "      <td>212559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75630</td>\n",
       "      <td>172257</td>\n",
       "      <td>219611</td>\n",
       "      <td>211554</td>\n",
       "      <td>248722</td>\n",
       "      <td>250977</td>\n",
       "      <td>216926</td>\n",
       "      <td>190967</td>\n",
       "      <td>415428</td>\n",
       "      <td>419254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8  \\\n",
       "0  60432  174457  184174  111847  172257  227812  250885  174558  212559   \n",
       "1  65407  225043  138191  177902  176620  225371  190349  147590  181198   \n",
       "2  75149   60432  133587  148584  186787  222725  212087  174369  423155   \n",
       "3  75629  205372  186058  171392  160160  184174  172257   75630  226747   \n",
       "4  75630  172257  219611  211554  248722  250977  216926  190967  415428   \n",
       "\n",
       "        9  \n",
       "0  214967  \n",
       "1  138245  \n",
       "2  431690  \n",
       "3  212559  \n",
       "4  419254  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "film_csv = \"/Volumes/Seagate/natasha-diploma/content_data.csv\"\n",
    "trailers_csv = \"/Volumes/Seagate/natasha-diploma/trailers.csv\"\n",
    "sessions_csv = \"/Volumes/Seagate/natasha-diploma/sessions.csv\"\n",
    "\n",
    "films = pd.read_csv(film_csv, index_col=None, header=0)\n",
    "display(films.head())\n",
    "\n",
    "trailers_meta = pd.read_csv(trailers_csv, index_col=None, header=0)\n",
    "display(trailers_meta.head())\n",
    "\n",
    "\n",
    "\n",
    "sessions = pd.read_csv(sessions_csv, index_col=None, header=0)\n",
    "sessions.drop(sessions.columns[sessions.columns.str.contains('Unnamed',case = False)],axis = 1, inplace = True)\n",
    "        \n",
    "display(sessions.head())\n",
    "\n",
    "sess = sessions.to_numpy().tolist()\n",
    "\n",
    "in_sess = []\n",
    "out_sess = []\n",
    "for i in range(len(sess)):\n",
    "    in_sess.append(sess[i][:5])\n",
    "    out_sess.append(sess[i][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l): return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]\n",
    "\n",
    "def vocab_creater(arr, add_num):\n",
    "    json_arr = []\n",
    "    for i in range(len(arr)):\n",
    "        js = json.loads(arr[i].replace('\\'', '\\\"'))\n",
    "        json_arr.append(js)\n",
    "    flat_arr = flatten(json_arr)\n",
    "    dic = list(dict.fromkeys(flat_arr))\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for i in range(len(dic)):\n",
    "        word2idx[dic[i]] = i + add_num\n",
    "        idx2word[i + add_num] = dic[i]\n",
    "\n",
    "    return word2idx, idx2word, len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailers_meta_array = []\n",
    "\n",
    "time_of_act = trailers_meta['Time of act'].values\n",
    "place_of_act = trailers_meta['Place of act'].to_numpy()\n",
    "qualities = trailers_meta['Qualities'].to_numpy()\n",
    "based_on = trailers_meta['Based on'].to_numpy()\n",
    "content_id = trailers_meta['content_id'].values\n",
    "audience = trailers_meta['Audience'].to_numpy()\n",
    "mood = trailers_meta['Mood'].to_numpy()\n",
    "subgenre = trailers_meta['Subgenre'].to_numpy()\n",
    "about = trailers_meta['About'].to_numpy()\n",
    "theme = trailers_meta['Theme'].to_numpy()\n",
    "\n",
    "add_num = 0\n",
    "time_vocab = vocab_creater(time_of_act, add_num)\n",
    "add_num += time_vocab[2] + 10\n",
    "place_vocab = vocab_creater(place_of_act, add_num)\n",
    "add_num += place_vocab[2]\n",
    "qua_vocab = vocab_creater(qualities, add_num)\n",
    "add_num += qua_vocab[2]\n",
    "based_vocab = vocab_creater(based_on, add_num)\n",
    "add_num += based_vocab[2]\n",
    "audi_vocab = vocab_creater(audience, add_num)\n",
    "add_num += audi_vocab[2]\n",
    "mood_vocab = vocab_creater(mood, add_num)\n",
    "add_num += mood_vocab[2]\n",
    "subgenr_vocab = vocab_creater(subgenre, add_num)\n",
    "add_num += subgenr_vocab[2]\n",
    "about_vocab = vocab_creater(about, add_num)\n",
    "add_num += about_vocab[2]\n",
    "theme_vocab = vocab_creater(theme, add_num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_arr(arr, vocab):\n",
    "    json_arr = []\n",
    "    for i in range(len(arr)):\n",
    "        js = json.loads(arr[i].replace('\\'', '\\\"'))\n",
    "        json_arr.append(js)\n",
    "    vocabs_arr = []\n",
    "    for i in range(len(json_arr)):\n",
    "        j_vocabs_arr = []\n",
    "        for j in range(len(json_arr[i])):\n",
    "            value = json_arr[i][j]\n",
    "            j_vocabs_arr.append(vocab[value])\n",
    "        vocabs_arr.append(j_vocabs_arr)\n",
    "            \n",
    "    return vocabs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_time = tokenize_arr(time_of_act, time_vocab[0])\n",
    "tok_place = tokenize_arr(place_of_act, place_vocab[0])\n",
    "tok_quality = tokenize_arr(qualities, qua_vocab[0])\n",
    "tok_based_on = tokenize_arr(based_on, based_vocab[0])\n",
    "tok_audience = tokenize_arr(audience, audi_vocab[0])\n",
    "tok_mood = tokenize_arr(mood, mood_vocab[0])\n",
    "tok_subgenre = tokenize_arr(subgenre, subgenr_vocab[0])\n",
    "tok_about = tokenize_arr(about, about_vocab[0])\n",
    "tok_theme = tokenize_arr(theme, theme_vocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailers_meta_preprocessed = []\n",
    "in_sess_preprocessed = []\n",
    "out_sess_preprocessed = []\n",
    "for i in range(len(tok_time)):\n",
    "    trailers_meta_preprocessed.append([\n",
    "#         content_id[i],\n",
    "        tok_time[i],\n",
    "        tok_place[i],\n",
    "        tok_quality[i],\n",
    "        tok_based_on[i],\n",
    "        tok_audience[i],\n",
    "        tok_mood[i],\n",
    "        tok_subgenre[i],\n",
    "        tok_about[i],\n",
    "        tok_theme[i]\n",
    "    ])\n",
    "\n",
    "    \n",
    "for i in range(len(in_sess)):\n",
    "    in_sess_preprocessed_j = []\n",
    "    for j in range(len(in_sess[i])):\n",
    "        ind = content_id.tolist().index(in_sess[i][j])\n",
    "        if ind:\n",
    "            in_sess_preprocessed_j.append([\n",
    "#                 content_id[ind],\n",
    "                tok_time[ind][0],\n",
    "                tok_place[ind][0],\n",
    "                tok_quality[ind][0],\n",
    "                tok_based_on[ind][0],\n",
    "                tok_audience[ind][0],\n",
    "                tok_mood[ind][0],\n",
    "                tok_subgenre[ind][0],\n",
    "                tok_about[ind][0],\n",
    "                tok_theme[ind][0]\n",
    "            ])\n",
    "    in_sess_preprocessed.append(in_sess_preprocessed_j)\n",
    "    \n",
    "for i in range(len(out_sess)):\n",
    "    out_sess_preprocessed_j = []\n",
    "    for j in range(len(out_sess[i])):\n",
    "        ind = content_id.tolist().index(out_sess[i][j])\n",
    "        if ind:\n",
    "            out_sess_preprocessed_j.append([\n",
    "#                 content_id[ind],\n",
    "                tok_time[ind][0],\n",
    "                tok_place[ind][0],\n",
    "                tok_quality[ind][0],\n",
    "                tok_based_on[ind][0],\n",
    "                tok_audience[ind][0],\n",
    "                tok_mood[ind][0],\n",
    "                tok_subgenre[ind][0],\n",
    "                tok_about[ind][0],\n",
    "                tok_theme[ind][0]\n",
    "            ])\n",
    "    out_sess_preprocessed.append(out_sess_preprocessed_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробовать с 3d массивом, надо дополнить недостающее нулями\n",
    "x = []\n",
    "for i in range(len(in_sess_preprocessed)):\n",
    "    for j in range(len(in_sess_preprocessed[i])):\n",
    "        x.append(in_sess_preprocessed[i][j])\n",
    "        \n",
    "y = []\n",
    "for i in range(len(out_sess_preprocessed)):\n",
    "    for j in range(len(out_sess_preprocessed[i])):\n",
    "        y.append(out_sess_preprocessed[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = 500000\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(249, return_sequences=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2,\n",
    "                     dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 9)\n",
      "(249, 9)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(x).shape)\n",
    "print(np.array(y[:-2]).shape)\n",
    "x = np.array(x).astype(np.float)\n",
    "y = np.array(y).astype(np.float)\n",
    "x_train = np.reshape(x, (x.shape[0], x.shape[1], 1))\n",
    "y_train = np.reshape(y[:-2], (y[:-2].shape[0], y[:-2].shape[1], 1))\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "# history = model.fit(np.asarray(x_train)[:200], np.asarray(y_train)[:200], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6d0f53f6e58c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0mbase_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_api_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m     \u001b[0mversion_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisallow_legacy_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[0m_disallow_inside_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2590\u001b[0m     \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2591\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2592\u001b[0;31m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[1;32m   2593\u001b[0m                          \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2594\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "model.evaluate(np.asarray(x_train)[200:], np.asarray(y_train)[200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:219 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 9)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-64d767207bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# вывод\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:219 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 9)\n"
     ]
    }
   ],
   "source": [
    "model.predict(x) # вывод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(array, max_len, max_h):\n",
    "    new_arr = []\n",
    "    for i in range(max_len):\n",
    "        new_arr_j = []\n",
    "        if (i < np.array(array).shape[0]):\n",
    "            for j in range(max_h):\n",
    "                if j < np.array(array).shape[1]:\n",
    "                    new_arr_j.append(array[i][j])\n",
    "                else:\n",
    "                    new_arr_j.append(0)\n",
    "        else:\n",
    "            for j in range(max_h):\n",
    "                 new_arr_j.append(0)\n",
    "        new_arr.append(new_arr_j)\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/nutochkina/PycharmProjects/diploma/venv/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 396\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_array = []\n",
    "min_len = 100\n",
    "max_len = 0\n",
    "del_array_id = []\n",
    "info_folder = \"/Volumes/Seagate/natasha-diploma/videoinfo\"\n",
    "for i in range(0, len(trailers_meta)):\n",
    "    name = trailers_meta.iloc[i]['trailers_name']\n",
    "    content_id = trailers_meta.iloc[i]['content_id']\n",
    "    try:\n",
    "        info_csv = info_folder + '/' + name + '.csv'\n",
    "        info_df =  pd.read_csv(info_csv, index_col=None, header=0)\n",
    "        info_df = info_df.iloc[: , 3:]\n",
    "        np_info_df = info_df.to_numpy()\n",
    "        new_arr = []\n",
    "        for i in range(len(np_info_df)):\n",
    "            new_arr_j = []\n",
    "            for j in range(len(np_info_df[i])):\n",
    "                if isinstance(np_info_df[i][j], int) or isinstance(np_info_df[i][j], float):\n",
    "                    new_arr_j.append(np_info_df[i][j])\n",
    "                else:\n",
    "                    nums = [int(s) for s in np_info_df[i][j].split() if s.isdigit()]\n",
    "                    new_arr_j.append(float(np.mean(nums)))\n",
    "            new_arr.append(new_arr_j)\n",
    "            \n",
    "        info_array.append(preprocessing.normalize(new_arr))        \n",
    "#         print(info_array[i])\n",
    "        if len(info_array[i]) < min_len:\n",
    "            min_len = len(info_array[i])\n",
    "            min_name = name\n",
    "        if len(info_array[i]) > max_len:\n",
    "            max_len = len(info_array[i])\n",
    "            max_name = name\n",
    "    except:\n",
    "        del_array_id.append(content_id)\n",
    "        \n",
    "print(min_len, max_len)\n",
    "padded = []\n",
    "for i in range (len(info_array)):\n",
    "    print(i)\n",
    "    padded.append(pad(info_array[i], max_len, 23))\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(909, 396, 23)\n"
     ]
    }
   ],
   "source": [
    "shape = np.array(padded).shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "encoder_input = Input(shape=(shape[1], shape[2]))\n",
    "encoder_LSTM1 = LSTM(shape[0], return_sequences=True, dropout=0.2)\n",
    "encoder_outputs = encoder_LSTM1(encoder_input)\n",
    "encoder_LSTM2 = LSTM(249, return_state=True)\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM2(encoder_outputs)\n",
    "encoder_states = [encoder_h, encoder_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "decoder_input = Input(shape=(9, 1))\n",
    "decoder_LSTM1 = LSTM(249, return_sequences=True, dropout=0.2)\n",
    "decoder_output = decoder_LSTM1(decoder_input, initial_state=encoder_states)\n",
    "decoder_LSTM2 = LSTM(128, return_sequences=True, return_state=True)\n",
    "decoder_output, _, _ = decoder_LSTM2(decoder_output)\n",
    "decoder_dense = Dense(912, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
    "model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "x = np.asarray(x_train)[:200].tolist()\n",
    "y = np.asarray(y_train)[:200].tolist()\n",
    "\n",
    "for i in range(709):\n",
    "    x.append(np.asarray(np.zeros((9, 1))))\n",
    "    y.append(np.asarray(np.zeros((9, 1))))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/29 [=================>............] - ETA: 49s - loss: 0.6053"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-e7ea4bb2c0b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/diploma/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=[np.asarray(padded), np.asarray(x)], y=np.asarray(y), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded[888])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909\n",
      "909\n"
     ]
    }
   ],
   "source": [
    "x = np.asarray(x_train)[200:].tolist()\n",
    "y = np.asarray(y_train)[200:].tolist()\n",
    "\n",
    "for i in range(909-49):\n",
    "    x.append(np.asarray(np.zeros((9, 1))))\n",
    "    y.append(np.asarray(np.zeros((9, 1))))\n",
    "    \n",
    "print(len(x))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 25s 811ms/step - loss: 0.1766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17655621469020844"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([np.asarray(padded), np.asarray(x)], np.asarray(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict([np.asarray(padded), np.asarray(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7964889561292661, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.359775832257999e-05, 0.016662996671261403, 0.00012252277522148975, 2.457878069831108e-11, 5.7847643877053234e-11, -0.003590966504990844, 0.5410445232518892, -0.0023109506792094586, 3.546966767222693e-05, 2.2142612341759125e-06, 0.26940723492152124, 0], [0.796488860475226, 0.0, 0.0, 0.0, 0.0004900910420286535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.359775428766991e-05, 0.01666299467012512, 0.00012252276050716336, 2.4578777746531704e-11, 5.7847636929862314e-11, -0.0035909660737350794, 0.5410444582753517, -0.0023109504016767105, 3.5469663412510604e-05, 2.214260968255047e-06, 0.2694072025671612, 0], [0.796488860475226, 0.0, 0.0, 0.0, 0.0004900910420286535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.359775428766991e-05, 0.01666299467012512, 0.00012252276050716336, 2.4578777746531704e-11, 5.7847636929862314e-11, -0.0035909660737350794, 0.5410444582753517, -0.0023109504016767105, 3.5469663412510604e-05, 2.214260968255047e-06, 0.2694072025671612, 0], [0.7843836330432111, 0.007152481511498463, 0.002578910896534868, 0.0, 0.0005698908782652623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.754223569125326e-05, 0.02300920507644476, 0.00014247271956631556, 1.8110460815566701e-10, 8.12950977256932e-10, -0.003931866127683416, 0.5447641005890799, -0.0023319445640829373, 5.003523227155466e-05, 3.372191412679617e-06, 0.2955772491161647, 0], [0.8130710948566197, 0.006339095405551365, 0.00227677617361271, 0.0, 0.0002674398100089354, 0.0001337199050044677, 0.0, 0.0, 0.0001337199050044677, 0.0, 0.0, 5.400712721566015e-05, 0.021595634072376805, 0.0001337199050044677, 1.6997844269530438e-10, 7.630073166497456e-10, -0.0036903118483632624, 0.5112965064635349, -0.002188681500107897, 4.69613167109827e-05, 3.165020761399238e-06, 0.2774184545196266, 0], [0.8124398239723838, 0.006638052037681954, 0.002321754884655893, 0.0, 0.0, 0.0, 0.0005356854850817246, 0.0, 0.0, 0.0, 0.0, 5.408849591133181e-05, 0.0216281706775855, 0.00013392137127043115, 1.7023453712001221e-10, 7.641568854815742e-10, -0.003695871778639204, 0.5120668405282646, -0.002191979030787382, 4.703206998531514e-05, 3.1697892729718015e-06, 0.2778364211652023, 0], [0.7444612748641108, 0.005605281829064958, 0.010596653221688491, 0.0, 0.0, 0.0, 0.0006805414497918572, 0.0, 0.0, 0.0, 0.0, 8.681926302098945e-05, 0.01831779659168756, 0.0001701353624479643, -1.0611580816462712e-09, 8.662163060849845e-09, -0.00450168353335937, 0.5795011195564602, -0.0013664887069555012, 6.002506602179893e-05, 4.9639654420645355e-06, 0.3308382944138908, 0], [0.7231792234790486, 0.006085405297830363, 0.011355011180287793, 0.00017599272585798172, 0.0, 0.00035198545171596343, 0.00017599272585798172, 0.0, 0.0, 0.0, 0.0, 8.980824759883907e-05, 0.018948435572111802, 0.00017599272585798172, -1.0976912775101244e-09, 8.960381116368693e-09, -0.004656665989871559, 0.5994519904683925, -0.001413533723565562, 6.209158893826492e-05, 5.134863185664621e-06, 0.3422282846690042, 0], [0.7501652705721952, 0.00532523340081304, 0.010722963555384756, 0.0, 0.0, 0.0005055005758175309, 0.00016850019193917698, 0.0, 0.0, 0.0, 0.0, 8.598484331867746e-05, 0.018141744298138928, 0.00016850019193917698, -1.0509592941907826e-09, 8.578911035076892e-09, -0.004458417865083651, 0.5739315358621957, -0.0013533553876851588, 5.944816527444914e-05, 4.9162567836133225e-06, 0.3276586084601929, 0], [0.7650501274614323, 0.004933056196393628, 0.002078770788671558, 0.0, 0.0, 0.0003653399921686221, 0.0001217799973895407, 0.0, 0.0, 0.0, 0.0, 5.9637448953847065e-05, 0.010850952084210826, 0.0001217799973895407, 4.16993193735512e-11, -1.9718570352100327e-09, -0.002885629263285954, 0.5757136077161972, -0.0012492180973636255, 4.724005363469517e-05, 3.3198182116347585e-06, 0.28826322785549674, 0], [0.7417402066802907, 0.004963447382706311, 0.0017678413905327585, 0.0, 0.0, 0.00012683278351770667, 0.00038049835055312006, 0.0, 0.0, 0.0, 0.0, 6.211187235056727e-05, 0.011301170029224537, 0.00012683278351770667, 4.34294700304828e-11, -2.053671553914329e-09, -0.0030053571974714196, 0.5996005989563089, -0.0013010495311681638, 4.920009545449985e-05, 3.4575611231748567e-06, 0.30022358645453295, 0], [0.6107599279896909, 0.005067952071757874, 0.006338412221190725, 0.00014973600358736343, 0.0, 0.00014973600358736343, 0.00029947200717472686, 0.0, 0.0, 0.0, 0.0, 7.332791478004586e-05, 0.013341913573954865, 0.00014973600358736343, 5.127188018682734e-11, -2.424519612638258e-09, -0.003548058819028163, 0.7078753217126471, -0.001535990552782723, 5.808455404942783e-05, 4.081920859766998e-06, 0.35443738418065357, 0], [0.6941790885676229, 0.0049764470010834234, 0.001486391146322965, 0.0, 0.0003643077723104637, 0.0001214359241034879, 0.0, 0.0, 0.0, 0.0, 0.0, 3.521510230246258e-05, 0.01845814884982933, 0.0001214359241034879, -6.489362369642908e-11, -4.738542554586002e-10, -0.0032150200745149154, 0.6560564221289135, -0.001956124352846364, 5.012966317861654e-05, 2.324482352306713e-06, 0.2955052581391788, 0], [0.8293349943499388, 0.0018304886854426986, 0.0032416430658879446, 0.00018853174259597817, 0.0, 9.426587129798909e-05, 0.0, 0.0, 0.0, 9.426587129798909e-05, 0.0, 2.7336081360574182e-05, 0.014328325795868515, 9.426587129798909e-05, -5.0374335474357525e-11, -3.678341857142576e-10, -0.0024956920351378096, 0.5092704709021693, -0.0015184614260532591, 3.891366094747973e-05, 1.8044030699701158e-06, 0.22938896242834936, 0], [0.738948853631316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003410143866914586, 0.0001136714622304862, 0.0, 0.0, 3.296349248271673e-05, 0.017277957781586997, 0.0001136714622304862, -6.074440615053674e-11, -4.435566040261257e-10, -0.003009455691703181, 0.6141089908908822, -0.0018310521959154196, 4.692443489604439e-05, 2.1758578432728796e-06, 0.2766110196588272, 0], [0.7441817046402289, 0.002917304848639115, 0.005859471235186652, 0.0, 0.0005485317913074669, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.948389980756577e-05, 0.019686077471239655, 0.00013713294782686673, -2.326515834414682e-10, -1.5721856488154522e-10, -0.003949428008108604, 0.5968747272910488, -0.0026804810939418623, 4.852358726887876e-05, 2.5366096283645776e-06, 0.29913317788244553, 0], [0.7869350026581678, 0.003028534852950062, 0.0058230036025490655, 0.00012667384482313666, 0.0002533476896462733, 0.0, 0.00012667384482313666, 0.0, 0.0, 0.0, 0.0, 3.647247052218596e-05, 0.018184624208008874, 0.00012667384482313666, -2.1490729285517274e-10, -1.452275358090079e-10, -0.0036482059094283073, 0.5513512090411384, -0.002476041327238354, 4.482270279583123e-05, 2.343142910090468e-06, 0.27631834914226844, 0], [0.797316964200075, 0.00272204994293273, 0.00583253645075383, 0.00012390745363609026, 0.00012390745363609026, 0.0, 0.0, 0.0002478149072721805, 0.0, 0.0, 0.0, 3.56759594415974e-05, 0.017787495785649674, 0.00012390745363609026, -2.1021399849897299e-10, -1.4205595626360578e-10, -0.003568533861181377, 0.5393104193438781, -0.002421967821251396, 4.384383355753572e-05, 2.2919717318135725e-06, 0.27028391759127496, 0], [0.797476722206927, 0.004704832578125885, 0.0035995297218303023, 0.00015554151305478033, 0.00015554151305478033, 0.0, 0.0, 0.00031108302610956067, 0.0, 0.0, 0.0, 4.98979355019968e-05, 0.012965059335400353, 0.00015554151305478033, -2.36053659169098e-10, 7.565922658402651e-11, -0.00474811348367495, 0.5307383710575826, -0.0029450497110228907, 5.422003779677635e-05, 3.2036893629997675e-06, 0.28655380211470266, 0], [0.8461799438864607, 0.0031276898644187656, 0.00198619253789507, 0.0002747648270357096, 0.0001373824135178548, 0.0001373824135178548, 0.0, 0.0, 0.0, 0.0, 0.0, 4.4072470906260676e-05, 0.01145141967515889, 0.0001373824135178548, -2.0849495918784304e-10, 6.682619288489911e-11, -0.004193782593680937, 0.46877593595701855, -0.002601222202897626, 4.788997809817509e-05, 2.8296662942668998e-06, 0.25309933125937095, 0], [0.792337439745331, 0.005814846838007344, 0.003519725013492322, 0.00015727518514346454, 0.0, 0.00015727518514346454, 0.0, 0.00015727518514346454, 0.00015727518514346454, 0.0, 0.0, 5.045409993915455e-05, 0.013109568418901436, 0.00015727518514346454, -2.3868472294298013e-10, 7.65025274289525e-11, -0.004801036151449522, 0.5366540027254589, -0.0029778753559810294, 5.4824376563511685e-05, 3.239397816134662e-06, 0.2897477425546265, 0], [0.7340279549384042, 0.006297145166106206, 0.0035508758932874707, 0.0002601539915611535, 0.00013007699578057675, 0.00013007699578057675, 0.0, 0.0, 0.0, 0.0, 0.0, 5.6158277467457817e-05, 0.01527804202927051, 0.00013007699578057675, -2.2066985878279843e-10, -4.756176602395027e-10, -0.0030732173904198033, 0.6045554164721731, -0.0017319170883257655, 4.5489445552727475e-05, 3.3869254799966295e-06, 0.3088971682961565, 0], [0.7434116392222917, 0.008116289325479958, 0.0032822337346042397, 0.0, 0.0, 0.0, 0.0001281032948282369, 0.0002562065896564738, 0.0001281032948282369, 0.0, 0.0, 5.530616949053115e-05, 0.015046223282826475, 0.0001281032948282369, -2.1732156258469896e-10, -4.684009573680136e-10, -0.0030265864542283584, 0.5953822987039389, -0.0017056381418749387, 4.4799219264289196e-05, 3.3355345479931742e-06, 0.304210170171833, 0], [0.7894517042436392, 0.005552072515843053, 0.001180939798807542, 0.0, 0.0, 0.0, 0.0, 0.00035270983583229453, 0.00011756994527743151, 0.0, 0.0, 5.07585954695744e-05, 0.013809040980294911, 0.00011756994527743151, -1.9945220187308603e-10, -4.2988648339991977e-10, -0.002777724056810202, 0.5464267283025735, -0.0015653912982660824, 4.111558382972332e-05, 3.0612687581873105e-06, 0.27919635562766876, 0], [0.5862204204979176, 0.0023393374847415394, 0.003917326083172201, 0.0, 0.0, 0.0, 0.000248214888948, 0.000248214888948, 0.0, 0.0, 0.0, 4.925171298211214e-05, 0.012827667893679843, 0.000124107444474, -8.864271306147524e-11, -4.770597385471908e-10, -0.0022926159377213017, 0.7414983855300719, -0.0013734542906580739, 6.501276306230341e-05, 3.022589175252651e-06, 0.3260875773323306, 0], [0.5920135836261199, 0.0041456322132394675, 0.003748318737942362, 0.00012345941320642838, 0.00037037823961928517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.899454347766265e-05, 0.012760687786883185, 0.00012345941320642838, -8.817986210237545e-11, -4.745687548000968e-10, -0.0022806449651622167, 0.7376266263401707, -0.0013662827520876115, 6.467329669560674e-05, 3.006806622458266e-06, 0.3243848998904283, 0], [0.6226854933839918, 0.006409626830725259, 0.002593717359135318, 0.0002397288996154608, 0.0, 0.0002397288996154608, 0.0, 0.0, 0.0, 0.0, 0.0, 4.756789170633426e-05, 0.012389114616845885, 0.0001198644498077304, -8.56121893058894e-11, -4.607499842473853e-10, -0.002214235811236673, 0.7161479828351657, -0.0013264985318450064, 6.279010181839558e-05, 2.919252668701604e-06, 0.3149392706596579, 0], [0.48429943716488677, 0.006703333537593985, 0.010166498696893957, 0.0005123149424535912, 0.0, 0.0, 0.00017077164748453044, 0.0, 0.0, 0.0, 0.0, 7.886995044433364e-05, 0.022063563439651733, 0.00017077164748453044, -2.1650848367606314e-11, -7.561395811395134e-10, -0.004425340568029314, 0.7788716298783052, -0.0023848016540990264, 5.5583073771000017e-05, 4.336247287843912e-06, 0.3976837249438632, 0], [0.7746206628554554, 0.005781430158072277, 0.0010175550525086182, 0.00024689903599407916, 0.00024689903599407916, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.7014484021332495e-05, 0.01594958128028158, 0.00012344951799703958, -1.5651232710922393e-11, -5.466075206577375e-10, -0.0031990448540086384, 0.5630403448485521, -0.0017239548780703564, 4.0180578959662626e-05, 3.1346400031000032e-06, 0.28748252349111064, 0], [0.6294644424769299, 0.004322026813963731, 0.006200538494345763, 0.0, 0.0, 0.0, 0.0006067014887936649, 0.0, 0.0, 0.0, 0.0, 7.005044025237619e-05, 0.019596339591650847, 0.00015167537219841623, -1.9229775744042266e-11, -6.71585442271261e-10, -0.003930483705273147, 0.6917755148276619, -0.002118124898558673, 4.936758253686198e-05, 3.851353143311901e-06, 0.35321335764226963, 0], [0.774410937247933, 0.0037768317921032745, 0.0020315631306951553, 0.0003703405994918685, 0.0001234468664972895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.840654149670604e-05, 0.01678867232482669, 0.0001234468664972895, -1.7388691046848237e-10, -2.1381872856002188e-10, -0.0028851017110559734, 0.5631121067285132, -0.0015413501657490767, 4.116095395523496e-05, 3.689889713844178e-06, 0.287891178591938, 0], [0.7471270776763606, 0.0049649376486270705, 0.0018252849392732537, 0.0001296891464230854, 0.0001296891464230854, 0.0, 0.0001296891464230854, 0.0, 0.0001296891464230854, 0.0, 0.0, 7.186562306673024e-05, 0.017637617261281043, 0.0001296891464230854, -1.826797684921399e-10, -2.246308001412696e-10, -0.0030309912990691247, 0.5915867330964593, -0.001619290898238134, 4.3242320650816605e-05, 3.876474640158496e-06, 0.3024488370884996, 0], [0.7951963218006869, 0.004097573924680191, 0.0017387051226535547, 0.0, 0.0, 0.0, 0.0004732351724551875, 0.0, 0.0, 0.0, 0.0, 6.555934221094105e-05, 0.016089898570060986, 0.00011830879311379688, -1.6664943468828105e-10, -2.0491922102873427e-10, -0.0027650187577104445, 0.5396744010207887, -0.0014771964898722476, 3.944776343078206e-05, 3.5363100834763037e-06, 0.27590864680286226, 0], [0.7388540475197083, 0.0036716021147866745, 0.010611469338717613, 0.0, 0.0, 0.0, 0.0003205979320941121, 0.0, 0.0003205979320941121, 0.0, 0.0, 9.770129872265304e-05, 0.012944063237320263, 0.00016029896604705606, -1.8915719437574544e-10, -9.948430599745673e-10, -0.003964516836247181, 0.5906630580933229, -0.0018456588957118808, 5.393541218912429e-05, 5.2528162416510235e-06, 0.3238809163201583, 0], [0.737702602788987, 0.004472704184006516, 0.009163516197234307, 0.0, 0.0, 0.000321205504279239, 0.000321205504279239, 0.0, 0.0, 0.0, 0.0, 9.788645460050576e-05, 0.012968593815961705, 0.0001606027521396195, -1.8951567033087292e-10, -9.967284089157127e-10, -0.003972030079209907, 0.591782436632465, -0.0018491566444370261, 5.403762637380544e-05, 5.2627709691227925e-06, 0.324494710160824, 0], [0.7465632405943522, 0.003050059026710419, 0.013710669922871807, 0.0, 0.0, 0.0004747546151991783, 0.0001582515383997261, 0.0, 0.0, 0.0, 0.0, 9.64534033361907e-05, 0.012778734454518898, 0.0001582515383997261, -1.8674117336820768e-10, -9.82136370493187e-10, -0.003913879819808442, 0.5831187806396447, -0.0018220851126501004, 5.324651907390772e-05, 5.185724347893311e-06, 0.3197441289230281, 0], [0.7790290940495908, 0.004935513505727653, 0.0031933781679205266, 0.0, 0.0, 0.00044474153267772855, 0.0, 0.00014824717755924284, 0.0, 0.0, 0.0, 0.00011305858258508305, 0.015322735618037367, 0.00014824717755924284, 1.6521315914283412e-10, 1.737526671593792e-08, -0.0035852428271638194, 0.5436117083995151, -0.0009658123161158046, 4.481608141796582e-05, 5.436161500566867e-06, 0.3119560780004253, 0], [0.7866625095282334, 0.005688808903628006, 0.0027601594179360333, 0.0002919500600042169, 0.00014597503000210845, 0.0, 0.00014597503000210845, 0.0, 0.0, 0.0, 0.0, 0.00011132576185646501, 0.015087887866624178, 0.00014597503000210845, 1.6268097821275986e-10, 1.710896033174116e-08, -0.003530292703555439, 0.5352799071766823, -0.0009510095513627563, 4.41291965032532e-05, 5.35284280757681e-06, 0.30717480491157584, 0], [0.7968102725088907, 0.0052614496898534535, 0.0032490336897545006, 0.0002857299294753229, 0.0, 0.00014286496473766145, 0.00014286496473766145, 0.0, 0.0, 0.0, 0.0, 0.00010895391521267333, 0.014766433464681726, 0.00014286496473766145, 1.592149850253054e-10, 1.6744446049827455e-08, -0.0034550781911110856, 0.5238755221524588, -0.0009307478547437666, 4.3189003641564856e-05, 5.238798025522994e-06, 0.3006303041784369, 0], [0.8573488820845963, 0.00659361417794202, 0.0027742934180178293, 0.0004648082740623576, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0001549360913541192, 0.0, 0.000105755632888621, 0.016681351632609025, 0.0001549360913541192, 1.930042222997584e-10, -5.35195089931997e-09, -0.004499747615670704, 0.43269952703297865, -0.0011282359095325892, 4.422413568547727e-05, 5.462383296060848e-06, 0.27815962301830865, 0], [0.7178023938800259, 0.006959965984553461, 0.011877650919454764, 0.00020955043070177071, 0.0, 0.0, 0.0006286512921053121, 0.0, 0.0, 0.0, 0.0, 0.00014303406151054741, 0.022561459946162325, 0.00020955043070177071, 2.6103742231199255e-10, -7.238491730657771e-09, -0.006085890270446614, 0.5852243429000282, -0.0015259344592306477, 5.9812962875927925e-05, 7.387851096175706e-06, 0.37620975395658884, 0], [0.7693592407092374, 0.006667255287324142, 0.007786895620134604, 0.0007691130694720609, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00013124450009362724, 0.020701834938792654, 0.00019227826736801522, 2.395214512910981e-10, -6.6418601177173394e-09, -0.005584261662810547, 0.5369873127798435, -0.0014001595365632378, 5.488288823561772e-05, 6.778908559568773e-06, 0.3452007204922865, 0], [1.0, 4.764636221680232e-127, 7.130215529763397e-127, 9.984100626554886e-129, 2.9952301879664657e-128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.746941392913888e-129, 1.2899380008722769e-126, 9.984100626554886e-129, 2.5418876803524805e-134, 9.0579052676883e-134, -2.5235659469088446e-127, 3.3014138772061815e-125, -5.297940127200011e-128, 3.268382130735778e-129, 3.5284786032543575e-130, 2.0174605739272608e-125, 0], [1.0, 2.2382420445452043e-90, 2.3238944268361537e-90, 0.0, 0.0, 0.0, 0.0, 4.019215810719168e-92, 1.2057647432157504e-91, 0.0, 0.0, 2.7160597168131947e-92, 5.192795427325644e-90, 4.019215810719168e-92, 1.0232664449286735e-97, 3.646365098430643e-97, -1.0158908180704482e-90, 1.3290225478800476e-88, -2.1327474070975347e-91, 1.3157252342175224e-92, 1.4204300938498998e-93, 8.12152214758774e-89, 0], [1.0, 2.2702978011887218e-82, 2.373476997509569e-82, 0.0, 0.0, 0.0, 0.0, 4.131206857491735e-84, 1.2393620572475205e-83, 0.0, 0.0, 2.7917397462287554e-84, 5.337486984825748e-82, 4.131206857491735e-84, 1.0517786437484478e-89, 3.747967068446563e-89, -1.044197503125509e-82, 1.3660543056484363e-80, -2.192174077838958e-83, 1.3523864769037508e-84, 1.4600088227783775e-85, 8.347819467644358e-81, 0], [0.8431285053843524, 1.6832384554466202e-07, 0.003229924191195151, 0.0, 0.0, 0.0, 0.0, 0.00016149620955975758, 0.0004844886286792727, 0.0, 0.0, 5.1994232268157924e-05, 0.02086518410620696, 0.00016149620955975758, 6.582609799542706e-11, 1.5262277662085819e-09, -0.005979896413284387, 0.45175537018173273, -0.002445642852844811, 4.507432860994321e-05, 2.8452023998831193e-06, 0.2907981922874836, 0], [0.8900512853812054, 0.0, 0.0, 0.0, 0.0005476613466397439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.408033993352404e-05, 0.017689354531356963, 0.00013691533665993597, 5.5806897218345856e-11, 1.2939250339053204e-09, -0.0050697136041042605, 0.38299498647662744, -0.0020733985983944245, 3.821369487973896e-05, 2.4121423376287436e-06, 0.2465366370249649, 0], [0.8900512853812054, 0.0, 0.0, 0.0, 0.0005476613466397439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.408033993352404e-05, 0.017689354531356963, 0.00013691533665993597, 5.5806897218345856e-11, 1.2939250339053204e-09, -0.0050697136041042605, 0.38299498647662744, -0.0020733985983944245, 3.821369487973896e-05, 2.4121423376287436e-06, 0.2465366370249649, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[[9.24264193e-01 2.28945143e-03 8.59670155e-03 ... 9.70430733e-07\n",
      "   7.14388875e-07 1.02104741e-06]\n",
      "  [9.50043835e-03 3.12480843e-04 3.75439879e-04 ... 9.00592568e-06\n",
      "   7.11065695e-06 1.00714906e-05]\n",
      "  [2.74568796e-03 3.41735395e-05 4.49996332e-05 ... 2.58900423e-06\n",
      "   2.63693505e-06 2.76329183e-06]\n",
      "  ...\n",
      "  [8.41026660e-04 6.96624120e-05 1.18675329e-04 ... 4.46496597e-06\n",
      "   5.97407825e-06 5.06383776e-06]\n",
      "  [4.06840729e-04 1.06518957e-04 1.08549699e-04 ... 1.74435863e-05\n",
      "   1.53156561e-05 1.96696583e-05]\n",
      "  [6.54172269e-04 5.18835695e-05 4.74554145e-05 ... 2.30987052e-06\n",
      "   1.62998685e-06 2.16716785e-06]]\n",
      "\n",
      " [[9.24264193e-01 2.28944933e-03 8.59669223e-03 ... 9.70429824e-07\n",
      "   7.14388250e-07 1.02104639e-06]\n",
      "  [9.50043555e-03 3.12480668e-04 3.75439646e-04 ... 9.00592931e-06\n",
      "   7.11066650e-06 1.00714851e-05]\n",
      "  [2.74567353e-03 3.41734194e-05 4.49995205e-05 ... 2.58900764e-06\n",
      "   2.63693846e-06 2.76329001e-06]\n",
      "  ...\n",
      "  [8.46324197e-04 6.97677315e-05 1.19005490e-04 ... 4.45566775e-06\n",
      "   5.95970414e-06 5.05337448e-06]\n",
      "  [4.06739447e-04 1.06521948e-04 1.08658176e-04 ... 1.74574470e-05\n",
      "   1.53538531e-05 1.97090121e-05]\n",
      "  [6.55134616e-04 5.20840804e-05 4.76383138e-05 ... 2.32495972e-06\n",
      "   1.64105734e-06 2.18156811e-06]]\n",
      "\n",
      " [[9.24264193e-01 2.28945143e-03 8.59670155e-03 ... 9.70430733e-07\n",
      "   7.14388250e-07 1.02104741e-06]\n",
      "  [8.86391010e-03 2.99483305e-04 3.89008637e-04 ... 9.60039233e-06\n",
      "   7.52436836e-06 1.04217443e-05]\n",
      "  [2.49431166e-03 3.29428913e-05 4.41807351e-05 ... 2.69357497e-06\n",
      "   2.71721342e-06 2.82597489e-06]\n",
      "  ...\n",
      "  [8.17791093e-04 6.92529866e-05 1.17617121e-04 ... 4.51968754e-06\n",
      "   6.03677381e-06 5.10997279e-06]\n",
      "  [4.02051257e-04 1.06221582e-04 1.08112945e-04 ... 1.75143887e-05\n",
      "   1.53312503e-05 1.97114023e-05]\n",
      "  [6.51852053e-04 5.18077832e-05 4.74102890e-05 ... 2.31120202e-06\n",
      "   1.62839024e-06 2.16441731e-06]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[9.24264193e-01 2.28944933e-03 8.59669223e-03 ... 9.70429824e-07\n",
      "   7.14388250e-07 1.02104639e-06]\n",
      "  [9.51649010e-01 3.35483463e-04 6.33595802e-04 ... 3.14633496e-07\n",
      "   2.35524226e-07 3.39095379e-07]\n",
      "  [9.87267077e-01 7.86345845e-05 1.94054577e-04 ... 5.11604661e-08\n",
      "   4.02550029e-08 6.70973392e-08]\n",
      "  ...\n",
      "  [9.98831809e-01 2.02013780e-05 8.91272139e-05 ... 2.78547230e-09\n",
      "   2.71734235e-09 3.76286113e-09]\n",
      "  [9.98862028e-01 2.01400471e-05 8.76762278e-05 ... 2.67525913e-09\n",
      "   2.64139444e-09 3.65603881e-09]\n",
      "  [9.98868108e-01 2.00903687e-05 8.70819422e-05 ... 2.67059019e-09\n",
      "   2.62990851e-09 3.64409347e-09]]\n",
      "\n",
      " [[9.24264193e-01 2.28944933e-03 8.59669223e-03 ... 9.70429824e-07\n",
      "   7.14388250e-07 1.02104639e-06]\n",
      "  [9.51649129e-01 3.35483026e-04 6.33595220e-04 ... 3.14632899e-07\n",
      "   2.35523800e-07 3.39094441e-07]\n",
      "  [9.87267077e-01 7.86343517e-05 1.94054213e-04 ... 5.11603702e-08\n",
      "   4.02549283e-08 6.70970763e-08]\n",
      "  ...\n",
      "  [9.98831809e-01 2.02013780e-05 8.91272139e-05 ... 2.78546697e-09\n",
      "   2.71734235e-09 3.76285403e-09]\n",
      "  [9.98862028e-01 2.01400471e-05 8.76761478e-05 ... 2.67525913e-09\n",
      "   2.64138444e-09 3.65603881e-09]\n",
      "  [9.98868108e-01 2.00903687e-05 8.70819422e-05 ... 2.67059019e-09\n",
      "   2.62990851e-09 3.64409347e-09]]\n",
      "\n",
      " [[9.24264312e-01 2.28944956e-03 8.59669317e-03 ... 9.70429937e-07\n",
      "   7.14388307e-07 1.02104650e-06]\n",
      "  [9.51649129e-01 3.35483172e-04 6.33595220e-04 ... 3.14633212e-07\n",
      "   2.35523800e-07 3.39094754e-07]\n",
      "  [9.87267077e-01 7.86345845e-05 1.94054577e-04 ... 5.11604661e-08\n",
      "   4.02550029e-08 6.70973392e-08]\n",
      "  ...\n",
      "  [9.98831809e-01 2.02013380e-05 8.91269592e-05 ... 2.78546697e-09\n",
      "   2.71734235e-09 3.76283937e-09]\n",
      "  [9.98862028e-01 2.01400653e-05 8.76763079e-05 ... 2.67526423e-09\n",
      "   2.64139444e-09 3.65603881e-09]\n",
      "  [9.98868108e-01 2.00903887e-05 8.70820222e-05 ... 2.67059508e-09\n",
      "   2.62990851e-09 3.64410035e-09]]]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0], [45.0], [187.0], [221.0], [247.0], [275.0], [314.0], [374.0], [888.0]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
